{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CartPole Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YBUjQ96xxib",
        "outputId": "6461bbae-a1dc-4a0c-992f-34cd3695f611"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import moviepy.editor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E6Cd3AeUerbl"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "class CartPoleBot:\n",
        "\n",
        "  env:gym.Env\n",
        "  learningRate:float\n",
        "  discountFactor:float\n",
        "\n",
        "  def __init__(self, env: gym.Env, learningRate: float,\n",
        "               initalEpsilon: float, epsilonDecay: float, finalEpsilon: float,\n",
        "               discountFactor: float):\n",
        "\n",
        "    self.env = env # cartpole environment.\n",
        "\n",
        "    self.learningRate = learningRate # rate at which update values in Q-table\n",
        "\n",
        "    self.epsilon = initalEpsilon\n",
        "    self.epsilonDecay = epsilonDecay\n",
        "    self.finalEpsilon = finalEpsilon\n",
        "\n",
        "\n",
        "    self.qTable = defaultdict(lambda: np.zeros(self.env.action_space.n)) #creates Q-table\n",
        "  \n",
        "    self.discountFactor = discountFactor\n",
        "\n",
        "  def discConv(self, obs):\n",
        "    '''\n",
        "    takes a numpy array representing the simulation state and returns a\n",
        "    hashable tuple with  values \"rounded\" to the closest chunk\n",
        "    '''\n",
        "    #DO NOT CHANGE.\n",
        "    posSpace = np.linspace(-2.4, 2.4, 10)\n",
        "    velSpace = np.linspace(-4, 4, 10)\n",
        "    angSpace = np.linspace(-.2095, .2095, 10)\n",
        "    angVSpace = np.linspace(-4, 4, 10)\n",
        "    lTodArray = [posSpace, velSpace, angSpace, angVSpace]\n",
        "    tR = []\n",
        "    for i in range(len(obs)):\n",
        "      tR += [np.digitize(obs[i], lTodArray[i])]\n",
        "\n",
        "    return(tuple(tR))\n",
        "\n",
        "  def getAction(self, observation):\n",
        "    #TO DO\n",
        "    '''\n",
        "    function generates random number, checks if it's higher than epsilon and\n",
        "    then based on that chooses a random action or look up the Q-table's reccomended action.\n",
        "    '''\n",
        "\n",
        "    x = np.random.rand()\n",
        "    state = self.discConv(observation)\n",
        "\n",
        "    if x > self.epsilon:\n",
        "      action = self.env.action_space.sample()\n",
        "    else:\n",
        "      action = np.argmax(self.qTable[state])\n",
        "    return action\n",
        "\n",
        "\n",
        "  def update(self, pastObv, action, reward, terminated, currObv):\n",
        "    '''\n",
        "    adjusting our q values based on how good/bad the action was\n",
        "\n",
        "    pastObv: State of the simulation before we took an action.\n",
        "    action: action we took \n",
        "    reward: the reward given to us by the environment\n",
        "    terminated: whether the simulation ended or not because we failed \n",
        "    currObv: state of the simulation after the action from getAction was taken.\n",
        "    '''\n",
        "    pastObv = self.discConv(pastObv)\n",
        "    currObv = self.discConv(currObv)\n",
        "\n",
        "    if terminated:\n",
        "      q_val = 0\n",
        "    else:\n",
        "      q_val = max(self.qTable[currObv])\n",
        "\n",
        "    temporalDiff = (reward) + (q_val * self.discountFactor) - self.qTable[pastObv][action]\n",
        "\n",
        "    self.qTable[pastObv][action] += self.learningRate * temporalDiff\n",
        "\n",
        "\n",
        "\n",
        "  def decayEpsilon(self):\n",
        "    #TO DO\n",
        "    '''\n",
        "    used to decay epsilon overtime\n",
        "    '''\n",
        "\n",
        "    #Your code here:\n",
        "    if self.epsilon > self.epsilonDecay:\n",
        "      self.epsilon = self.epsilonDecay\n",
        "    else:\n",
        "      self.epsilon = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-okFDFJyJ-5",
        "outputId": "beca125e-c021-416b-c2cd-994b6e085183"
      },
      "outputs": [],
      "source": [
        "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode = \"rgb_array\"), \"/content\", episode_trigger= lambda x: (x%5000 == 0), new_step_api= True)\n",
        "\n",
        "#\"example\" parameter used - not perfect and could use finetuning in future \n",
        "learningRate = 0.05\n",
        "nEps = 60_000\n",
        "startEpsilon = 1.0\n",
        "epsilonDecay = (1.0/30_000.0)\n",
        "finalEpsilon = 0.1\n",
        "discountFactor = 0.95\n",
        "\n",
        "balanceAgent = CartPoleBot(env, learningRate, startEpsilon, epsilonDecay, finalEpsilon, discountFactor)\n",
        "\n",
        "for i in tqdm(range(nEps)):\n",
        "  observation, info = env.reset() #reset the environment at the start of every episode\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = balanceAgent.getAction(observation)\n",
        "    newObv, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if i % 5000 == 0:\n",
        "      env.render()\n",
        "\n",
        "    balanceAgent.update(observation, action, reward, terminated, newObv)\n",
        "\n",
        "    done = terminated or truncated\n",
        "    observation = newObv\n",
        "\n",
        "  balanceAgent.decayEpsilon() #always decaying epsilon!\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "daKctNGlvixI",
        "outputId": "93ee4b51-3221-48d4-c5ea-bd79da326128"
      },
      "outputs": [],
      "source": [
        "moviepy.editor.ipython_display(\"/content/rl-video-episode-60000.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wxS__amLU9v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d90899474ca7564da09ac0137e471eb753504361ee90ceb7aed4d4473ab7128c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
